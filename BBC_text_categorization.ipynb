{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BBC text categorization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yskZKyjPSkTU",
        "colab_type": "text"
      },
      "source": [
        "# BBC text categorization\n",
        "\n",
        "Tập dữ liệu này chứa 2225 bài báo trên BBC và gồm 2 trường. Trường thứ nhất là chủ đề của bài báo, trường thứ 2 là nội dung và tiêu đề của bài báo. Trong bài này m sẽ sử dụng machine learning cũng như convolutional neural network để phân loại bài báo vào 1 trong 5 chủ đề."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFn7kAG5WDhv",
        "colab_type": "text"
      },
      "source": [
        "## Download data\n",
        "\n",
        "Các bạn có thể tải sữ liệu tại đây [BBC articles fulltext and category](https://www.kaggle.com/yufengdev/bbc-fulltext-and-category)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqGYzl0sR_NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "raw_data = pd.read_csv('/content/drive/My Drive/Google colab data/bbc-text.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnpbYxpMWa8n",
        "colab_type": "text"
      },
      "source": [
        "## Understanding data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgCPTR6wSgCU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "781d2874-fe3e-4085-de23-26895d61fd8d"
      },
      "source": [
        "raw_data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        category                                               text\n",
              "0           tech  tv future in the hands of viewers with home th...\n",
              "1       business  worldcom boss  left books alone  former worldc...\n",
              "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3          sport  yeading face newcastle in fa cup premiership s...\n",
              "4  entertainment  ocean s twelve raids box office ocean s twelve..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiUhRnRuWpHB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "085afa7b-4136-4109-cae2-076751f2e59f"
      },
      "source": [
        "raw_data['text'][1] # category: business"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an $11bn (£5.8bn) fraud  never made accounting decisions  a witness has told jurors.  david myers made the comments under questioning by defence lawyers who have been arguing that mr ebbers was not responsible for worldcom s problems. the phone company collapsed in 2002 and prosecutors claim that losses were hidden to protect the firm s shares. mr myers has already pleaded guilty to fraud and is assisting prosecutors.  on monday  defence lawyer reid weingarten tried to distance his client from the allegations. during cross examination  he asked mr myers if he ever knew mr ebbers  make an accounting decision  .  not that i am aware of   mr myers replied.  did you ever know mr ebbers to make an accounting entry into worldcom books   mr weingarten pressed.  no   replied the witness. mr myers has admitted that he ordered false accounting entries at the request of former worldcom chief financial officer scott sullivan. defence lawyers have been trying to paint mr sullivan  who has admitted fraud and will testify later in the trial  as the mastermind behind worldcom s accounting house of cards.  mr ebbers  team  meanwhile  are looking to portray him as an affable boss  who by his own admission is more pe graduate than economist. whatever his abilities  mr ebbers transformed worldcom from a relative unknown into a $160bn telecoms giant and investor darling of the late 1990s. worldcom s problems mounted  however  as competition increased and the telecoms boom petered out. when the firm finally collapsed  shareholders lost about $180bn and 20 000 workers lost their jobs. mr ebbers  trial is expected to last two months and if found guilty the former ceo faces a substantial jail sentence. he has firmly declared his innocence.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkfRHG0YX8gk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "917c1299-587c-43e7-e12d-5893a705a982"
      },
      "source": [
        "raw_data.isnull().sum()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "category    0\n",
              "text        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfOLy5t3YLJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "cbb76c49-92dd-4b18-824c-fa39bfbf2f2f"
      },
      "source": [
        "raw_data.describe()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2225</td>\n",
              "      <td>2225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>5</td>\n",
              "      <td>2126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>sport</td>\n",
              "      <td>howl helps boost japan s cinemas japan s box o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>511</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       category                                               text\n",
              "count      2225                                               2225\n",
              "unique        5                                               2126\n",
              "top       sport  howl helps boost japan s cinemas japan s box o...\n",
              "freq        511                                                  2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImBj6XowY2jI",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ9xEtVrYnZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk \n",
        "import re\n",
        "from nltk.stem import PorterStemmer \n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "def preprocessing_data(text):\n",
        "\n",
        "  # Loại bỏ những kí tự đặc biệt\n",
        "  REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "  BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "\n",
        "  text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "  text = BAD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "  # Stemming and lemantization\n",
        "  stemmer = PorterStemmer() # SnowballStemmer hỗ trợ 15 ngôn ngữ ko phải tiếng anh nhưng ko có tiếng việt\n",
        "  text = stemmer.stem(text)\n",
        "\n",
        "  # Remove stop-words\n",
        "  text = remove_stopwords(text)\n",
        "  text = text.strip()\n",
        "\n",
        "  return text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxOREBYCdrSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "27903fed-72e0-43ac-a6c2-1f642c331535"
      },
      "source": [
        "data = raw_data.copy()\n",
        "data['text'].apply(lambda text: preprocessing_data(text))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       tv future hands viewers home theatre systems p...\n",
              "1       worldcom boss left books worldcom boss bernie ...\n",
              "2       tigers wary farrell gamble leicester rushed ma...\n",
              "3       yeading face newcastle fa cup premiership newc...\n",
              "4       ocean s raids box office ocean s crime caper s...\n",
              "                              ...                        \n",
              "2220    cars pull retail figures retail sales fell 03 ...\n",
              "2221    kilroy unveils immigration policy exchatshow h...\n",
              "2222    rem announce new glasgow concert band rem anno...\n",
              "2223    political squabbles snowball s commonplace arg...\n",
              "2224    souness delight euro progress boss graeme soun...\n",
              "Name: text, Length: 2225, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEqe4E9DebgT",
        "colab_type": "text"
      },
      "source": [
        "## Feature extraction and training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDFNrFQReakF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test, y_train,y_test = train_test_split(data['text'], data['category'], train_size = 0.9 , random_state=42 )"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuJRrPLxn5DO",
        "colab_type": "text"
      },
      "source": [
        "### Using CountVectorizer với LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaEB0sLNE0or",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "collapsed": true,
        "outputId": "e5b53a23-2504-4870-ba7b-1e0272ab085d"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Minh sẽ không sử dụng tree model ở đây vì không phù hợp với bài toán này\n",
        "\n",
        "# Using CountVectorizer với LogisticRegression\n",
        "pipeline = Pipeline ([('BOW',CountVectorizer()),('clf' , LogisticRegression() )])\n",
        "\n",
        "params = {\n",
        "    'BOW__ngram_range':[(1,2),(1,3)],\n",
        "    'BOW__min_df': [0.01, 0.05, 0.1], # Loai bỏ những cặp từ ít xuất hiện (df: document Frequentcy)\n",
        "    'clf__penalty':['l2','l1'],\n",
        "    'clf__C':[0.001, 0.01, 0.1]\n",
        "}\n",
        "\n",
        "grid_logistic = GridSearchCV(pipeline, param_grid = params, n_jobs=-1, cv = 5)\n",
        "\n",
        "grid_logistic.fit(X_train,y_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('BOW',\n",
              "                                        CountVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.int64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        preprocessor=None,\n",
              "                                                        stop_words=None,\n",
              "                                                        strip_accents=None,\n",
              "                                                        token_pattern='(?u)\\...\n",
              "                                                           n_jobs=None,\n",
              "                                                           penalty='l2',\n",
              "                                                           random_state=None,\n",
              "                                                           solver='lbfgs',\n",
              "                                                           tol=0.0001,\n",
              "                                                           verbose=0,\n",
              "                                                           warm_start=False))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'BOW__min_df': [0.01, 0.05, 0.1],\n",
              "                         'BOW__ngram_range': [(1, 2), (1, 3)],\n",
              "                         'clf__C': [0.001, 0.01, 0.1],\n",
              "                         'clf__penalty': ['l2', 'l1']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2vrppGTmY7r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d966c60c-d23c-4a18-e399-200d20ccf67e"
      },
      "source": [
        "print('Best score of logistic with BOW: {}'.format(grid_logistic.best_score_))\n",
        "print('Best parameter of logistic with BOW: {}'.format(grid_logistic.best_params_))\n",
        "\n",
        "best_logistic_estimator = grid_logistic.best_estimator_"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score of logistic with BOW: 0.9685411471321697\n",
            "Best parameter of logistic with BOW: {'BOW__min_df': 0.01, 'BOW__ngram_range': (1, 2), 'clf__C': 0.1, 'clf__penalty': 'l2'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xGVYn7IoDAv",
        "colab_type": "text"
      },
      "source": [
        "### Using TfidfVectorizer with svc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVI8xfYyif6c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "6f89bfce-4e97-45ab-d770-9f2caac134ec"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "pipe = Pipeline([('Tfidf',TfidfVectorizer()), ('clf', SVC())])\n",
        "\n",
        "params2 = {\n",
        "    'Tfidf__ngram_range':[(1,2),(1,3)],\n",
        "    'Tfidf__min_df':[0.01, 0.05, 0.1],\n",
        "    'clf__C':[0.001, 0.01, 0.1],\n",
        "    'clf__kernel':['rbf','linear']\n",
        "}\n",
        "\n",
        "grid_SVC = GridSearchCV(pipe, param_grid= params2, n_jobs= -1, cv=5)\n",
        "grid_SVC.fit(X_train, y_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('Tfidf',\n",
              "                                        TfidfVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.float64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        norm='l2',\n",
              "                                                        preprocessor=None,\n",
              "                                                        smooth_idf=True,\n",
              "                                                        stop_words=None,\n",
              "                                                        strip_acc...\n",
              "                                            kernel='rbf', max_iter=-1,\n",
              "                                            probability=False,\n",
              "                                            random_state=None, shrinking=True,\n",
              "                                            tol=0.001, verbose=False))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'Tfidf__min_df': [0.01, 0.05, 0.1],\n",
              "                         'Tfidf__ngram_range': [(1, 2), (1, 3)],\n",
              "                         'clf__C': [0.001, 0.01, 0.1],\n",
              "                         'clf__kernel': ['rbf', 'linear']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NSEGJrIoL1G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "76151659-0b50-4f08-adbb-143387806705"
      },
      "source": [
        "print('Best score of SVC with Tfidf: {}'.format(grid_SVC.best_score_))\n",
        "print('Best parameter of SVC with Tfidf: {}'.format(grid_SVC.best_params_))\n",
        "\n",
        "best_SVC_estimator = grid_SVC.best_estimator_"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score of SVC with Tfidf: 0.9425561097256857\n",
            "Best parameter of SVC with Tfidf: {'Tfidf__min_df': 0.01, 'Tfidf__ngram_range': (1, 2), 'clf__C': 0.1, 'clf__kernel': 'linear'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPl8CfCsoHkb",
        "colab_type": "text"
      },
      "source": [
        "### Using Convolution neural network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isOeWAkIojbJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c2fe041a-f5ef-47fd-a672-b539cebd8b7d"
      },
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Đầu tiên ta biểu diễn mỗi từ bằng một số nguyên(index)\n",
        "# Vì embedding layer sẽ chuyển mỗi số nguyên(từ) thành một dense vecto\n",
        "tokenizer = Tokenizer(num_words = 40000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_cnn = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_cnn = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "print(X_train_cnn[1])\n",
        "print(X_train[1])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1689, 1733, 806, 2, 1830, 555, 510, 1893, 1689, 1733, 1, 1003, 539, 2131, 17, 2012, 2, 16, 5, 806, 2, 1, 1830, 1689, 9, 9225, 1, 64, 2184, 3, 1, 1422, 81, 29, 211, 255, 6, 1, 63, 770, 6, 1, 6561, 1831, 15, 13, 26, 1975, 66, 16, 1832, 60, 658, 35, 38, 954, 22, 1, 63, 1014, 3, 1, 409, 2184, 4, 1, 332, 35, 2, 146, 75, 828, 67, 26, 20, 2, 2520, 71, 9, 5758, 4, 1, 1003, 539, 9, 5758, 21, 1, 914, 26, 206, 1, 63, 510, 2, 20, 149, 11, 1689, 386, 6562, 2012, 9226, 173, 611, 5423, 5142, 4, 5759, 1753, 18523, 24, 15, 9, 28, 18524, 4726, 26, 222, 66, 20, 2, 16, 9227, 52, 2012, 101, 30, 20, 634, 79, 3, 829, 4, 20, 149, 1, 174, 330, 536, 26, 20, 38, 2217, 8, 171, 79, 4, 26, 20, 1, 275, 968, 3, 1003, 539, 5424, 2, 124, 664, 2012, 325, 14196, 853, 26, 150, 10, 11, 9, 133, 2, 16, 5, 659, 332, 8, 49, 101, 555, 20, 5, 130, 233, 423, 3, 234, 776, 26, 20, 302, 555, 5, 353, 337, 12, 696, 9228, 30, 25, 100, 130, 4, 30, 25, 100, 380, 12, 1, 3413, 895, 30, 708, 100, 110, 4, 466, 4211, 3, 1, 760, 76, 11, 23, 16, 390, 10, 31, 151, 123, 207, 93, 26, 404, 30, 25, 1, 68, 4921, 1833, 6, 237, 4922, 2012, 25, 10353, 2, 374, 234, 138, 21, 1, 6561, 1831, 190, 30, 20, 2, 392, 1, 2416, 3, 291, 1102, 422, 383, 11, 9, 5, 761, 93, 1, 2962, 6, 1, 579, 745, 51, 25, 2567, 3, 2132, 24, 1559, 1, 4923, 1584, 51, 9, 28, 5, 310, 3, 1621, 14196, 116, 31, 25, 83, 1924, 3, 443, 1, 332, 17, 58, 1952, 2, 1, 1151]\n",
            "worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an $11bn (£5.8bn) fraud  never made accounting decisions  a witness has told jurors.  david myers made the comments under questioning by defence lawyers who have been arguing that mr ebbers was not responsible for worldcom s problems. the phone company collapsed in 2002 and prosecutors claim that losses were hidden to protect the firm s shares. mr myers has already pleaded guilty to fraud and is assisting prosecutors.  on monday  defence lawyer reid weingarten tried to distance his client from the allegations. during cross examination  he asked mr myers if he ever knew mr ebbers  make an accounting decision  .  not that i am aware of   mr myers replied.  did you ever know mr ebbers to make an accounting entry into worldcom books   mr weingarten pressed.  no   replied the witness. mr myers has admitted that he ordered false accounting entries at the request of former worldcom chief financial officer scott sullivan. defence lawyers have been trying to paint mr sullivan  who has admitted fraud and will testify later in the trial  as the mastermind behind worldcom s accounting house of cards.  mr ebbers  team  meanwhile  are looking to portray him as an affable boss  who by his own admission is more pe graduate than economist. whatever his abilities  mr ebbers transformed worldcom from a relative unknown into a $160bn telecoms giant and investor darling of the late 1990s. worldcom s problems mounted  however  as competition increased and the telecoms boom petered out. when the firm finally collapsed  shareholders lost about $180bn and 20 000 workers lost their jobs. mr ebbers  trial is expected to last two months and if found guilty the former ceo faces a substantial jail sentence. he has firmly declared his innocence.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFjF085zDALF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "feb1235d-9ced-4610-b77e-8a7fb38095b4"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)\n",
        "\n",
        "max_len = X_train.apply(lambda x:len(x.split())).max()\n",
        "print(max_len)\n",
        "\n",
        "X_train_cnn = pad_sequences(X_train_cnn, padding='post', maxlen=max_len)\n",
        "X_test_cnn = pad_sequences(X_test_cnn, padding='post', maxlen=max_len)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28407\n",
            "4492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4QlB8YPROU_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "e863423b-bea0-4c31-98ba-31025be2d492"
      },
      "source": [
        "# Build model\n",
        "from tensorflow.keras import layers\n",
        "embedding_dim = 300\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Embedding(input_dim= vocab_size,output_dim= embedding_dim, input_length=max_len ))\n",
        "model.add(layers.Conv1D(128,5, activation = 'relu'))\n",
        "model.add(layers.GlobalAveragePooling1D()) # tao 128 feature\n",
        "model.add(layers.Dense(64, activation ='relu'))\n",
        "model.add(layers.Dense(5))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 4492, 300)         8522100   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 4488, 128)         192128    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 8,722,809\n",
            "Trainable params: 8,722,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrAzXC5F7R9G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "21ea1a1e-a5af-4499-d2c7-bba5b6c4d4af"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_cnn = le.fit_transform(y_train)\n",
        "y_test_cnn = le.fit_transform(y_test)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_cnn,y_train_cnn,epochs=10, batch_size=32, validation_data=(X_test_cnn,y_test_cnn))\n",
        "# Nên sử dụng thêm early stopping\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "63/63 [==============================] - 241s 4s/step - loss: 1.5927 - accuracy: 0.2577 - val_loss: 1.5159 - val_accuracy: 0.4484\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 243s 4s/step - loss: 1.0651 - accuracy: 0.6928 - val_loss: 0.6923 - val_accuracy: 0.8027\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 243s 4s/step - loss: 0.4016 - accuracy: 0.8751 - val_loss: 0.4447 - val_accuracy: 0.8700\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 241s 4s/step - loss: 0.1809 - accuracy: 0.9500 - val_loss: 0.3082 - val_accuracy: 0.9283\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 240s 4s/step - loss: 0.0717 - accuracy: 0.9920 - val_loss: 0.2209 - val_accuracy: 0.9327\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 240s 4s/step - loss: 0.0298 - accuracy: 0.9980 - val_loss: 0.2027 - val_accuracy: 0.9327\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 240s 4s/step - loss: 0.0147 - accuracy: 0.9995 - val_loss: 0.1702 - val_accuracy: 0.9507\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 240s 4s/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.1725 - val_accuracy: 0.9417\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 241s 4s/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.1695 - val_accuracy: 0.9462\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 244s 4s/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.1803 - val_accuracy: 0.9372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z_naLJbFxcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(X_train_cnn, y_train_cnn)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test_cnn, y_test_cnn)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57tJWtTAF6K4",
        "colab_type": "text"
      },
      "source": [
        "## Look at Feature importance\n",
        "\n",
        "Trong cac mô hình, có thể thấy logistic cho kết quả tốt nhất. Do đó ta sẽ chon mô hình best_logistic_estimator để tìm các feature tốt nhất. Vì các feature gần như có cùng scale nên ta sẽ sử dụng hệ số của mô hình logistic để đo lường mức độ quan trọng của feature. Hệ số càng lớn thì feature càng quan trọng"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSNk_HgxF5IQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0d7e9453-f53e-4403-b410-81c0a1a7ef47"
      },
      "source": [
        "coef_matrix = best_logistic_estimator[1].coef_\n",
        "\n",
        "for i in range(len(best_logistic_estimator[1].classes_)):\n",
        "  important_feature = sorted(enumerate(list(coef_matrix[i,:])), key = lambda x: x[1] ,reverse= True)[:10]\n",
        "  if_index = [i for i,v in important_feature]\n",
        "\n",
        "  vocab = best_logistic_estimator[0].vocabulary_.items()\n",
        "  important_feature1 = [i for i,v in vocab if v in if_index]\n",
        "  category = best_logistic_estimator[1].classes_[i]\n",
        "  print(category, ' : ', important_feature1 )\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "business  :  ['its', 'companies', 'bank', 'economy', 'economic', 'firm', 'business', 'company', 'market', 'shares']\n",
            "entertainment  :  ['star', 'us', 'tv', 'film', 'films', 'album', 'band', 'music', 'show', 'singer']\n",
            "politics  :  ['mps', 'mr', 'labour', 'government', 'party', 'secretary', 'uk', 'minister', 'election', 'blair']\n",
            "sport  :  ['but', 'win', 'club', 'match', 'cup', 'we', 'season', 'champion', 'olympic', 'seed']\n",
            "tech  :  ['game', 'games', 'computer', 'users', 'people', 'technology', 'software', 'digital', 'online', 'sony']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}